{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7f040-2283-4106-a5c1-f7cec6e3c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harper Adams University\n",
    "# Harper Food Innovation (HFI)\n",
    "\n",
    "# Production title: 'Bibliografix' (V0.1)\n",
    "\n",
    "# see https://github.com/glados-mcspud/bibliography for 'readme'\n",
    "\n",
    "# Striving for Open Access! \n",
    "\n",
    "# Note: comments may be excessive for established users of python; please note that this is intended to be used \n",
    "# in part for teaching and training purposes in addition to being a useful script for researchers/students/librarians and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c121f-24b9-4d3c-bb2f-fe0fb66dea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Ensure all necessary packages/modules from packages are installed ***\n",
    "# *** More may be added to future versions as new functions are provided/trialled ***\n",
    "\n",
    "# Essential/standard/common packages (see PyPi.org)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# *** For wordclouds and quantitative analyses later ***\n",
    "from matplotlib.colors import LinearSegmentedColormap  \n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "# Note: the purpose of a corpus is to provide a rich dataset for analysis, research, [...] \n",
    "# and training computational models, such as machine learning algorithms for language understanding\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Term Frequency (TF): Measures how often a given term appears in a document relative to the total number of terms in that document. \n",
    "# It reflects the importance of a term within a document.\n",
    "\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "# LDA: A probabilistic method for topic modeling. \n",
    "# It identifies hidden topics in a collection of documents by clustering words that frequently appear together.\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "# KMeans is a popular clustering algorithm for partitioning data into a predefined number of clusters (e.g., grouping similar documents).\n",
    "\n",
    "import nltk        # For advanced natural language processing tasks.\n",
    "import spacy       # For sophisticated text processing and entity recognition.\n",
    "import sklearn     # For statistical analyses, clustering, or topic modeling.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# (1) Tokenization: Splits the text into individual words or tokens.\n",
    "# (2) Vocabulary Building: Creates a dictionary of unique words across the aforementioned corpus (or corpora).\n",
    "# (3) Feature Matrix Generation: Constructs a sparse matrix (or dense array) where rows correspond to documents and columns correspond to unique words. \n",
    "# Each cell contains the count of a word's appearance in the document. This does NOT feature in the current version.\n",
    "\n",
    "# Standard packages/modules (see PyPi.org)\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "print(\"Active Conda environment:\", os.environ.get(\"CONDA_DEFAULT_ENV\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4683b04-602f-44ed-9502-18a69300c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Step 1: Dynamically identify and list your files ***\n",
    "# *** This will be selectable for the full user-friendly version ***\n",
    "files = glob.glob(\"search_*.xlsx\")\n",
    "if not files:\n",
    "    raise ValueError(\"No files found with the pattern 'search_*.xlsx'.\")\n",
    "\n",
    "def extract_number(filename):\n",
    "    match = re.search(r\"(\\d+)\", os.path.basename(filename))\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "files = sorted(files, key=extract_number)\n",
    "print(\"Files to process:\", files)\n",
    "\n",
    "# *** Step 2: Load the first file (with header) ***\n",
    "first_file = files[0]\n",
    "first_number = extract_number(first_file)\n",
    "sheet_name_first = f\"savedrecs_{first_number}\"\n",
    "df_first = pd.read_excel(first_file, sheet_name=sheet_name_first)\n",
    "lib_start = df_first.copy()  # Preserve the header from the first file\n",
    "\n",
    "# *** Step 3: Loop through the remaining files ***\n",
    "for file in files[1:]:\n",
    "    file_number = extract_number(file)\n",
    "    sheet_name = f\"savedrecs_{file_number}\"\n",
    "    # Read the file without its header (skip the first row)\n",
    "    df_temp = pd.read_excel(file, sheet_name=sheet_name, header=None, skiprows=1)\n",
    "    # Use the header from the first file\n",
    "    df_temp.columns = df_first.columns\n",
    "    # Concatenate this file's data into the master DataFrame\n",
    "    lib_start = pd.concat([lib_start, df_temp], ignore_index=True)\n",
    "\n",
    "# *** Step 4: Drop Empty Columns ***\n",
    "lib_start = lib_start.dropna(axis=1, how='all')\n",
    "\n",
    "# *** Step 5: Remove Duplicate Rows ***\n",
    "lib_start = lib_start.drop_duplicates(ignore_index=True)\n",
    "\n",
    "# *** Step 6: Prepare the environment for deeper literature analysis ***\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# *** Step 7: Check for empty rows (rows where all values are NaN) ***\n",
    "empty_rows = lib_start[lib_start.isnull().all(axis=1)]\n",
    "\n",
    "if empty_rows.empty:\n",
    "    print(\"No empty rows found in lib_start. Yay, let's dive in :)\")\n",
    "else:\n",
    "    print(f\"Found {empty_rows.shape[0]} empty row(s):\")\n",
    "    print(empty_rows)\n",
    "\n",
    "# *** Step 8: Print the full dimensions pre- and post deletion of non-journal entries for inspection ***\n",
    "print(\"Dimensions of lib_start dataframe (all entries):\", lib_start.shape) \n",
    "lib_start = lib_start[lib_start[\"Publication Type\"] == \"J\"].reset_index(drop=True) # remove non-article entries\n",
    "print(\"Dimensions of lib_start dataframe (only journal entries):\", lib_start.shape) #reprint\n",
    "    \n",
    "    # *** [8-A]: It looks like there are some non-English journals, let's check and, if so, remove them ***\n",
    "    # *** Rows where Language equals ('==') \"English\" will be kept; all others will be discarded ***\n",
    "    \n",
    "lib_start = lib_start[lib_start[\"Language\"] == \"English\"].reset_index(drop=True)\n",
    "    \n",
    "# *** Verify any removals by printing potentially new dimensions ***\n",
    "print(\"Dimensions after filtering non-English journals:\", lib_start.shape)\n",
    "\n",
    "# *** Step 9: Print all headers to make analysis easier later (e.g., we know exactly what datapoints we have to work with)\n",
    "print(\"Available headers in 'lib_start':\")\n",
    "list(lib_start.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50876ff2-242d-408e-9602-6bb1a72846ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "######VISUALS########\n",
    "# *** Step 1: Build the custom ordering based on publication counts ***\n",
    "journal_counts = lib_start[\"Source Title\"].value_counts()\n",
    "\n",
    "# Separate journals with more than one publication and those with exactly one.\n",
    "more_than_one = journal_counts[journal_counts > 1]\n",
    "one_pub = journal_counts[journal_counts == 1]\n",
    "\n",
    "# Order journals with >1 publication descending by count, then journals with one publication alphabetically.\n",
    "ordered_more = more_than_one.sort_values(ascending=False).index.tolist()\n",
    "ordered_one = sorted(one_pub.index.tolist())\n",
    "final_order = ordered_more + ordered_one\n",
    "\n",
    "# *** Step 2: Create a custom palette (smooth gradient from light green to dark green) ***\n",
    "# *** Normalize publication counts to the range [0, 1]. ***\n",
    "min_count = journal_counts.min()\n",
    "max_count = journal_counts.max()\n",
    "norm = Normalize(vmin=min_count, vmax=max_count)\n",
    "\n",
    "# *** Reverse the color gradient by setting the lower end to \"lightgreen\" and the upper end to \"darkgreen\". ***\n",
    "cmap = LinearSegmentedColormap.from_list(\"GreenGradient\", [\"lightgreen\", \"darkgreen\"])\n",
    "\n",
    "# *** Assign each journal a color based solely on its publication count. ***\n",
    "# *** Journals with the same count will receive the same color. ***\n",
    "journal_colors = {journal: cmap(norm(journal_counts[journal])) for journal in final_order}\n",
    "\n",
    "# *** Step 3: Plot the countplot with hue assigned (to enforce our custom palette) and disable the legend ***\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(data=lib_start,\n",
    "              y=\"Source Title\",\n",
    "              order=final_order,\n",
    "              hue=\"Source Title\",  # Use hue to apply our custom color mapping.\n",
    "              palette=journal_colors,\n",
    "              dodge=False)\n",
    "\n",
    "plt.legend([], [], frameon=False)  # Disable the legend.\n",
    "plt.title(\"Distribution of Articles by Journal\")\n",
    "plt.xlabel(\"Number of Articles\")\n",
    "plt.ylabel(\"\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"distribution_of_articles.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea2f77-924c-4542-b11a-d0b17f0ffd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Update the overall font size to 14\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "wos_counts = lib_start['WoS Categories'].value_counts()\n",
    "\n",
    "# Convert the Series into a DataFrame:\n",
    "df_counts = wos_counts.reset_index()\n",
    "df_counts.columns = ['Category', 'Count']\n",
    "\n",
    "# Create a new column \"CountLabel\" from the count values.\n",
    "# This ensures that categories with the same count get the same label and hence the same color.\n",
    "df_counts['CountLabel'] = df_counts['Count'].astype(str)\n",
    "\n",
    "# Set a larger figure size so that the plot matches the title length\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the count bar plot, using \"CountLabel\" for hue; this makes sure identical counts have the same color.\n",
    "sns.barplot(\n",
    "    x='Count', \n",
    "    y='Category', \n",
    "    hue='CountLabel',\n",
    "    data=df_counts, \n",
    "    palette=\"coolwarm\", \n",
    "    dodge=False, \n",
    "    legend=False\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Articles\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title(\"Distribution of Articles by WoS Categories\", loc=\"left\")\n",
    "#plt.tight_layout()\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762ac17-9f90-4d5e-93f7-97839f812523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Filter rows where \"WoS Categories\" contains \"Dance\" (case insensitive) ***\n",
    "# - that's an interesting result warranting exploration!\n",
    "dance_rows = lib_start[lib_start[\"WoS Categories\"].str.contains(\"Dance\", case=False, na=False)]\n",
    "\n",
    "# Print the resulting rows\n",
    "print(dance_rows)\n",
    "\n",
    "# Print the abstract from the row with index 50\n",
    "print(lib_start.loc[50, \"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc29b6-9d1e-4849-81a4-4257c9f2442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Step 1: Preprocess Abstracts (Remove duplicate words per abstract and filter out short words) ***\n",
    "abstracts = lib_start[\"Abstract\"].dropna().tolist()\n",
    "processed_abstracts = []\n",
    "for abstract in abstracts:\n",
    "    words = abstract.split()\n",
    "    # Only keep words with more than four characters\n",
    "    filtered_words = [word for word in words if len(word) > 4]\n",
    "    # Remove duplicate words within this abstract (order is not preserved)\n",
    "    unique_words = set(filtered_words)\n",
    "    processed_abstracts.append(\" \".join(unique_words))\n",
    "    \n",
    "# Combine all processed abstracts into one large text string\n",
    "combined_text = \" \".join(processed_abstracts)\n",
    "\n",
    "# *** Step 2: Define Stopwords using NLTK's English Stopwords ***\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# *** Step 3: Generate the word cloud for abstracts ***\n",
    "wordcloud = WordCloud(width=800, \n",
    "                      height=400, \n",
    "                      background_color='white', \n",
    "                      stopwords=stop_words, \n",
    "                      max_words=40, \n",
    "                      contour_width=10, \n",
    "                      contour_color='darkgrey', \n",
    "                      collocations=False).generate(combined_text)\n",
    "\n",
    "# *** Step 4: Display the word cloud ***\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d073b5-fad0-478c-a4a4-b45f4131b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# *** Step 1: Define search terms/patterns and custom labels ***\n",
    "search_patterns = [r\"food\", r\"agri\\w*\", r\"feed\\w*\", r\"nutrit\\w*\", r\"diet\\w*\"]\n",
    "custom_labels = [\"Food\", \"Agri*\", \"Feed*\", \"Nutrit*\", \"Diet*\"]  # Custom labels corresponding to search patterns\n",
    "\n",
    "# *** Step 2: Specify the columns to be searched ***\n",
    "columns_to_search = [\"Article Title\", \"Abstract\", \"Author Keywords\", \"Keywords Plus\"]\n",
    "\n",
    "# *** Step 3: Count frequency of each search term across the specified columns ***\n",
    "# Initialise a dictionary to hold the overall counts for each pattern.\n",
    "frequency_counts = {pattern: 0 for pattern in search_patterns}\n",
    "\n",
    "# Initialise a nested dictionary to hold counts per column for each search pattern.\n",
    "frequency_counts_by_column = {pattern: {col: 0 for col in columns_to_search} for pattern in search_patterns}\n",
    "\n",
    "# Iterate through each specified column and each pattern using nested 'for' loops to find matches to search words\n",
    "for col in columns_to_search:\n",
    "    if col in lib_start.columns:\n",
    "        # Replace missing values with empty strings to avoid errors\n",
    "        col_series = lib_start[col].fillna(\"\")\n",
    "        for pattern in search_patterns:\n",
    "            count = col_series.str.count(pattern, flags=re.IGNORECASE).sum()\n",
    "            frequency_counts[pattern] += count\n",
    "            frequency_counts_by_column[pattern][col] = count\n",
    "\n",
    "# *** Step 4: Print the overall frequency counts for each search term ***\n",
    "print(\"Overall Frequency Counts for each Search Term:\")\n",
    "for label, pattern in zip(custom_labels, search_patterns):\n",
    "    print(f\"Label '{label}': {frequency_counts[pattern]}\")\n",
    "\n",
    "# *** Step 5: Visualisation Option 1 - Bar Chart of Overall Frequency Counts ***\n",
    "patterns = list(frequency_counts.keys())\n",
    "counts = [frequency_counts[p] for p in patterns]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(custom_labels, counts, color='skyblue')\n",
    "plt.title(\"Overall Frequency of Each Search Term\")\n",
    "# plt.xlabel(\"Search Term\") \n",
    "plt.ylabel(\"Frequency Count\")\n",
    "plt.show()\n",
    "\n",
    "# *** Step 6: Visualisation Option 2 - Stacked Bar Chart by Column ***\n",
    "# Convert the nested dictionary into a DataFrame for easier plotting.\n",
    "freq_df = pd.DataFrame(frequency_counts_by_column).T  # Rows: search patterns, Columns: search columns\n",
    "\n",
    "# Replace row index names with custom labels\n",
    "freq_df.index = custom_labels\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "freq_df.plot(kind='bar', stacked=True, figsize=(10, 6), colormap=plt.cm.Dark2)\n",
    "\n",
    "# Rotate x-axis labels diagonally for improved legibility\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.title(\"Frequency of Each Search Term by Column\")\n",
    "# plt.xlabel(\"Search Term\")\n",
    "plt.ylabel(\"Frequency Count\")\n",
    "plt.legend(title=\"Column\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8894313-4dca-48af-95c4-163fc129b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import squarify  # For the treemap visualisation\n",
    "# from matplotlib_venn import venn2  # (Not used in final visualisation)\n",
    "\n",
    "# *** Step 1: Define search terms/patterns and custom label mapping ***\n",
    "search_patterns = [r\"food\", r\"agri\\w*\", r\"feed\\w*\", r\"nutri\\w*\", r\"diet\\w*\"]\n",
    "\n",
    "# Define custom labels for each search pattern\n",
    "custom_label_map = {\n",
    "    r\"food\": \"Food\",\n",
    "    r\"agri\\w*\": \"Agri*\",\n",
    "    r\"feed\\w*\": \"Feed*\",\n",
    "    r\"nutri\\w*\": \"Nutri*\",\n",
    "    r\"diet\\w*\": \"Diet*\"\n",
    "}\n",
    "\n",
    "# Create sorted lists based on the custom labels (alphabetical order)\n",
    "sorted_custom = sorted(custom_label_map.items(), key=lambda x: x[1])\n",
    "sorted_patterns = [item[0] for item in sorted_custom]  # e.g. [r\"agri\\w*\", r\"diet\\w*\", r\"feed\\w*\", r\"food\", r\"nutrit\\w*\"]\n",
    "sorted_labels   = [item[1] for item in sorted_custom]  # e.g. [\"Agri*\", \"Diet*\", \"Feed*\", \"Food\", \"Nutrit*\"]\n",
    "\n",
    "# *** Step 2: Specify the columns to be searched ***\n",
    "columns_to_search = [\"Article Title\", \"Abstract\", \"Author Keywords\", \"Keywords Plus\"]\n",
    "\n",
    "# *** Step 3: Count frequency of each search term across the specified columns ***\n",
    "# Initialise a dictionary to hold the overall counts for each pattern.\n",
    "frequency_counts = {pattern: 0 for pattern in search_patterns}\n",
    "# Initialise a nested dictionary to hold counts per column for each search pattern.\n",
    "frequency_counts_by_column = {pattern: {col: 0 for col in columns_to_search} for pattern in search_patterns}\n",
    "\n",
    "# Loop through each specified column and each pattern, counting all occurrences.\n",
    "for col in columns_to_search:\n",
    "    if col in lib_start.columns:\n",
    "        # Replace missing values with empty strings to avoid errors.\n",
    "        col_series = lib_start[col].fillna(\"\")\n",
    "        for pattern in search_patterns:\n",
    "            # Series.str.count treats the pattern as a regex by default.\n",
    "            count = col_series.str.count(pattern, flags=re.IGNORECASE).sum()\n",
    "            frequency_counts[pattern] += count\n",
    "            frequency_counts_by_column[pattern][col] = count\n",
    "\n",
    "# *** Step 4: Print the overall frequency counts for each search term ***\n",
    "print(\"Overall Frequency Counts for each Search Term:\")\n",
    "for pattern in sorted_patterns:\n",
    "    print(f\"Pattern '{custom_label_map[pattern]}': {frequency_counts[pattern]}\")\n",
    "\n",
    "# *** Step 5: Visualisation 1: Stacked bar chart by column ***\n",
    "# Convert the nested dictionary into a DataFrame and re-order rows based on our sorted_patterns.\n",
    "freq_df = pd.DataFrame(frequency_counts_by_column).T\n",
    "freq_df = freq_df.reindex(sorted_patterns)\n",
    "# Replace the row index names with our custom labels.\n",
    "freq_df.index = sorted_labels\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "freq_df.plot(kind='bar', stacked=True, figsize=(10,6), colormap='Paired')\n",
    "plt.title(\"Frequency of Each Search Term per Source\")\n",
    "plt.xlabel(\"Search Term\")\n",
    "plt.ylabel(\"Frequency Count\")\n",
    "plt.legend(title=\"Column\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# *** Step 6: Visualisation Option - Treemap of Overall Search Term Frequencies ***\n",
    "# Filter out search terms with zero frequency to avoid errors.\n",
    "treemap_data = [(custom_label_map[pattern], frequency_counts[pattern])\n",
    "                for pattern in sorted_patterns if frequency_counts[pattern] > 0]\n",
    "\n",
    "if treemap_data:\n",
    "    treemap_labels, sizes = zip(*treemap_data)\n",
    "    # Create labels that show both the custom label and frequency.\n",
    "    treemap_labels = [f\"{label}\\n({int(size)})\" for label, size in zip(treemap_labels, sizes)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    squarify.plot(sizes=sizes, label=treemap_labels, \n",
    "                  color=['lightblue', 'lightgreen', 'khaki', 'salmon', 'plum'][:len(sizes)], alpha=0.8)\n",
    "    plt.title(\"Treemap of Overall Search Term Frequencies\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No nonzero frequencies available for treemap visualisation.\")\n",
    "\n",
    "# *** Step 7: Print indices, authors, article titles, and abstracts for all matching records, including primary matching term ***\n",
    "# First, create an overall mask that flags any record that matches any of the search patterns in the specified columns.\n",
    "mask = pd.Series(False, index=lib_start.index)\n",
    "for col in columns_to_search:\n",
    "    if col in lib_start.columns:\n",
    "        for pattern in search_patterns:\n",
    "            mask |= lib_start[col].str.contains(pattern, flags=re.IGNORECASE, regex=True, na=False)\n",
    "\n",
    "results = lib_start[mask]\n",
    "\n",
    "print(\"\\nMatching Records:\")\n",
    "for idx, row in results.iterrows():\n",
    "    # For each row, count occurrences for each search pattern in the given columns.\n",
    "    pattern_counts = {}\n",
    "    for pattern in search_patterns:\n",
    "        total_count = 0\n",
    "        for col in columns_to_search:\n",
    "            if col in lib_start.columns:\n",
    "                text = str(row[col])\n",
    "                total_count += len(re.findall(pattern, text, flags=re.IGNORECASE))\n",
    "        pattern_counts[pattern] = total_count\n",
    "    \n",
    "    # Determine the primary matching term by choosing the pattern(s) with the highest count.\n",
    "    max_count = max(pattern_counts.values())\n",
    "    primary_terms = [custom_label_map[pattern] for pattern, count in pattern_counts.items() if count == max_count and count > 0]\n",
    "    primary_term_str = \", \".join(primary_terms) if primary_terms else \"None\"\n",
    "    \n",
    "    authors = row[\"Authors\"] if \"Authors\" in row and pd.notnull(row[\"Authors\"]) else \"No Authors Available\"\n",
    "    title = row[\"Article Title\"] if \"Article Title\" in row and pd.notnull(row[\"Article Title\"]) else \"No Title Available\"\n",
    "    abstract = row[\"Abstract\"] if \"Abstract\" in row and pd.notnull(row[\"Abstract\"]) else \"No Abstract Available\"\n",
    "    \n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Authors: {authors}\")\n",
    "    print(f\"Article Title: {title}\")\n",
    "    print(f\"Abstract: {abstract}\")\n",
    "    print(f\"Primary Matching Term: {primary_term_str}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cdf020-ae9c-4203-857f-5497ec7781c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# *** Step 1: Subset the Data for Selected Indices ***\n",
    "selected_indices = [17, 22, 24, 40, 41, 47, 53, 54, 58, 65, 66, 69, 78]\n",
    "df_subset = lib_start.loc[selected_indices].copy()\n",
    "# For further analyses we will work primarily with the 'Abstract' text field.\n",
    "# Fill in missing abstracts with an empty string.\n",
    "documents = df_subset[\"Abstract\"].fillna(\"\")\n",
    "\n",
    "# *** Step 2: Text Vectorization with TF-IDF ***\n",
    "# We use TF-IDF to convert our abstracts into numerical feature vectors.\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.85)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# *** Step 3a: Clustering with KMeans ***\n",
    "# Choose a number of clusters \n",
    "# Following trials, 5 (or 3) clusters seem sensible when working with abstract-level information; we are using 5 here\n",
    "# There may be a benefit to increasing clusters if scanning entire and multiple documents (!!TBC!!)\n",
    "num_clusters = 5\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans_model.fit_predict(X)\n",
    "# Add cluster labels for reference.\n",
    "df_subset[\"Cluster\"] = cluster_labels\n",
    "\n",
    "# *** Step 3b: Dimensionality Reduction for Visualisation ***\n",
    "# Use PCA to reduce the high-dimensional TF-IDF vectors to 2 dimensions.\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "# Plot the clusters on a 2D scatter plot.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0],\n",
    "                y=X_pca[:, 1],\n",
    "                hue=cluster_labels,\n",
    "                palette=\"deep\",\n",
    "                s=100)\n",
    "\n",
    "# Annotate each point with its original index\n",
    "for i, idx in enumerate(df_subset.index):\n",
    "    plt.text(X_pca[i, 0] + 0.01, X_pca[i, 1] + 0.01, str(idx), fontsize=9)\n",
    "\n",
    "plt.title(\"PCA of Abstracts with KMeans Clusters\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# *** Step 4: Topic Modeling with Latent Dirichlet Allocation (LDA) ***\n",
    "# Apply LDA to extract latent topics among the selected articles.\n",
    "n_topics = 5\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        # Get indices of the top words and map them to words.\n",
    "        top_features_ind = topic.argsort()[:-num_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        print(f\"Topic {topic_idx}: {' '.join(top_features)}\")\n",
    "\n",
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Extracted Topics:\")\n",
    "display_topics(lda, feature_names, num_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3dc2ff-f972-4360-98ca-da172ce7f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "# *** Step 1: Subset the Data for Selected Indices ***\n",
    "# Select a subset of document indices from our input data (i.e., the ones identified as focussing on agri-food system LCA/enviro pedagogy).\n",
    "selected_indices = [17, 22, 24, 40, 41, 47, 53, 54, 58, 65, 66, 69, 78]\n",
    "df_subset = lib_start.loc[selected_indices].copy()\n",
    "# For further analyses let's work primarily with the 'Abstract' field as it provide most relevant prose.\n",
    "# Missing abstracts are filled with an empty string.\n",
    "documents = df_subset[\"Abstract\"].fillna(\"\")\n",
    "\n",
    "# *** Step 2: Text Vectorisation with TF-IDF ***\n",
    "# Convert our abstracts into numerical feature vectors.\n",
    "# Remove common English stopwords and ignore terms appearing in more than 85% of documents (this seems to be commonly adopted on StackExchange etc.).\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.85)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# *** Step 3a: Clustering with KMeans ***\n",
    "# Define the desired number of clusters (here 5) and perform clustering.\n",
    "num_clusters = 5\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans_model.fit_predict(X)\n",
    "df_subset[\"Cluster\"] = cluster_labels\n",
    "\n",
    "# *** Step 3b: Print Each Extracted Cluster ***\n",
    "# Now print out each cluster's document indices so these can be used for the image caption.\n",
    "print(\"Extracted Clusters (document indices):\")\n",
    "for i in range(num_clusters):\n",
    "    # Get the document indices for the current cluster.\n",
    "    cluster_docs = df_subset[df_subset[\"Cluster\"] == i].index.tolist()\n",
    "    # Add 1 to the cluster label for presentation (i.e. cluster numbering begins at 1).\n",
    "    print(f\"Cluster {i+1}: {', '.join(str(doc) for doc in cluster_docs)}\")\n",
    "\n",
    "# *** Step 3c: Dimensionality Reduction for Visualisation ***\n",
    "# Reduce our high-dimensional TF-IDF vectors to 2 dimensions with PCA, for visualisation purposes.\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "# Begin plot creation with a fixed figure size.\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Create a scatter plot using Seaborn,\n",
    "# where each point is coloured according to its cluster.\n",
    "sns.scatterplot(x=X_pca[:, 0],\n",
    "                y=X_pca[:, 1],\n",
    "                hue=cluster_labels,\n",
    "                palette=\"deep\",\n",
    "                s=100,\n",
    "                legend=False,\n",
    "                ax=ax)\n",
    "\n",
    "# Annotate each point with its original index.\n",
    "for i, idx in enumerate(df_subset.index):\n",
    "    plt.text(X_pca[i, 0] + 0.01, X_pca[i, 1] + 0.01, str(idx), fontsize=9)\n",
    "\n",
    "plt.title(\"PCA of Abstracts with KMeans Clusters\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "\n",
    "# *** Step 3d: Create a Simple Legend ***\n",
    "# Now create a basic legend showing the cluster number (starting at 1).\n",
    "palette = sns.color_palette(\"deep\", num_clusters)\n",
    "handles = [Patch(facecolor=palette[i], label=f\"Cluster {i+1}\") for i in range(num_clusters)]\n",
    "plt.legend(handles=handles,\n",
    "           loc='upper center',\n",
    "           bbox_to_anchor=(0.5, -0.1),\n",
    "           ncol=num_clusters)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# *** Step 4: Topic Modelling with Latent Dirichlet Allocation (LDA) ***\n",
    "# Apply LDA to extract latent topics among the selected articles.\n",
    "n_topics = 5\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    # For each topic, list the top words to give a feel for its content.\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-num_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind if len(feature_names[i]) >= 4]\n",
    "        print(f\"Topic {topic_idx}: {' '.join(top_features)}\")\n",
    "\n",
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Extracted Topics:\")\n",
    "display_topics(lda, feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8701c757-d769-44d4-8ef0-37de79a437f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 15\n",
    "\n",
    "# Let's try make this more sensible!!\n",
    "\n",
    "# *** Step 1: Subset the Data ***\n",
    "# Selected indices based on prior exploration.\n",
    "selected_indices = [17, 22, 24, 40, 41, 47, 53, 54, 58, 65, 66, 69, 78]\n",
    "df_subset = lib_start.loc[selected_indices].copy()\n",
    "\n",
    "# Combine \"Article Title\" and \"Abstract\" to capture the teaching narrative for this domain.\n",
    "# If either is missing, replace with an empty string.\n",
    "df_subset['Combined_Text'] = (\n",
    "    df_subset['Article Title'].fillna('') + '. ' + df_subset['Abstract'].fillna('')\n",
    ")\n",
    "\n",
    "# *** Step 2: Vectorize the Text using CountVectorizer *** \n",
    "# (preferred for LDA as it expects raw counts)\n",
    "\n",
    "# According to multiple discussions on stackexchange it's best to use a CountVectorizer with English stop words.\n",
    "# Adjust max_df/min_df to remove overly common or extremely rare terms (if needed).\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", max_df=0.85, min_df=2)\n",
    "count_data = count_vectorizer.fit_transform(df_subset['Combined_Text'])\n",
    "\n",
    "\n",
    "# *** Step 3: Topic Modeling with LDA ***\n",
    "\n",
    "# For our targeted narrative we choose a modest number of topics.\n",
    "# You might start with 3 topics and then adjust based on domain interpretation.\n",
    "n_topics = 3\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)\n",
    "lda_topic_matrix = lda_model.fit_transform(count_data)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper: Display Topics Function\n",
    "# -------------------------------\n",
    "def get_topic_keywords(model, feature_names, no_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        # Get indices of the top words for this topic\n",
    "        top_words_idx = topic.argsort()[:-no_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append(top_words)\n",
    "    return topics\n",
    "\n",
    "no_top_words = 10\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "topic_keywords = get_topic_keywords(lda_model, feature_names, no_top_words)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Annotate Each Document with a Dominant Topic\n",
    "# -------------------------------\n",
    "# For each document, determine which topic has the highest probability.\n",
    "dominant_topics = np.argmax(lda_topic_matrix, axis=1)\n",
    "max_topic_weights = np.max(lda_topic_matrix, axis=1)\n",
    "\n",
    "df_subset['Dominant_Topic'] = dominant_topics\n",
    "df_subset['Topic_Weight'] = max_topic_weights\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Produce a Legible Summary Table\n",
    "# -------------------------------\n",
    "# Build a summary table that reports:\n",
    "# - Original index (as in the full DataFrame)\n",
    "# - Article Title (for context)\n",
    "# - Dominant Topic & Topic Weight\n",
    "# - Top keywords for the assigned topic\n",
    "# - A snippet of the abstract for quick human interpretation\n",
    "summary_columns = ['Article Title', 'Dominant_Topic', 'Topic_Weight', 'Abstract']\n",
    "\n",
    "summary_table = df_subset[summary_columns].copy()\n",
    "summary_table['Topic_Keywords'] = df_subset['Dominant_Topic'].apply(\n",
    "    lambda t: \", \".join(topic_keywords[t])\n",
    ")\n",
    "summary_table['Abstract_Snippet'] = df_subset['Abstract'].fillna(\"\").apply(\n",
    "    lambda text: text if len(text) < 250 else text[:250] + \"...\"\n",
    ")\n",
    "\n",
    "# For clarity, print a neat summary for each document:\n",
    "print(\"Summary of Selected Documents and LDA Topics:\\n\")\n",
    "for idx, row in summary_table.iterrows():\n",
    "    # Use the original DataFrame's index value for clarity.\n",
    "    print(f\"Document (DataFrame Index): {idx}\")\n",
    "    print(f\"Article Title    : {row['Article Title']}\")\n",
    "    print(f\"Dominant Topic   : {row['Dominant_Topic']} (Weight: {row['Topic_Weight']:.2f})\")\n",
    "    print(f\"Topic Keywords   : {row['Topic_Keywords']}\")\n",
    "    print(\"Abstract Snippet :\")\n",
    "    print(row['Abstract_Snippet'])\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Optional: Visualize Topic Distribution across these selected documents\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Dominant_Topic', data=df_subset, palette=\"Set2\")\n",
    "plt.xlabel(\"Dominant Topic\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.title(\"Topic Distribution for Selected Documents\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Interpretation Guidance (disclaimer: assistance from Co-Pilot here):\n",
    "# -------------------------------\n",
    "# The printed summary shows for each document:\n",
    "#  - Which topic (0, 1, or 2) is most dominant along with the weight (a proxy for confidence).\n",
    "#  - The top words of that topic provide a cue to what theme it captures.\n",
    "#\n",
    "# Given our narrative focus, you might interpret:\n",
    "#  • A topic whose keywords include terms like \"lifecycle\", \"assessment\", \"sustain\" may represent teaching approaches to LCA.\n",
    "#  • Another topic with terms like \"carbon\", \"footprint\", \"environment\" may group around sustainability or carbon footprint assessments.\n",
    "#  • Further interpretation may be refined by reading the abstract snippets.\n",
    "#\n",
    "# You can refine this analysis by:\n",
    "#  - Adjusting the number of topics.\n",
    "#  - Re-processing the text with additional domain-specific stopwords or lemmatization.\n",
    "#  - Comparing these results with clustering (e.g., KMeans) for robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b62f5c-17d1-4c93-9d86-a4a22a855ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 16\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Subset the Data\n",
    "# -------------------------------\n",
    "# Selected indices based on prior interest.\n",
    "selected_indices = [17, 22, 24, 40, 41, 47, 53, 54, 58, 65, 66, 69, 78]\n",
    "df_subset = lib_start.loc[selected_indices].copy()\n",
    "\n",
    "# Combine \"Article Title\" and \"Abstract\" to capture the narrative.\n",
    "# (Missing values are replaced with empty strings.)\n",
    "df_subset['Combined_Text'] = (df_subset['Article Title'].fillna('') + '. ' +\n",
    "                               df_subset['Abstract'].fillna(''))\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Vectorize the Text using CountVectorizer \n",
    "# -------------------------------\n",
    "# We use CountVectorizer (which expects raw counts) to build the document-term matrix.\n",
    "# Adjust max_df and min_df as needed.\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", max_df=0.85, min_df=2)\n",
    "count_data = count_vectorizer.fit_transform(df_subset['Combined_Text'])\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Clustering with KMeans\n",
    "# -------------------------------\n",
    "# We choose 5 clusters to reflect 5 topic groupings.\n",
    "num_clusters = 5\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans_model.fit_predict(count_data)\n",
    "df_subset[\"Cluster\"] = cluster_labels\n",
    "\n",
    "# Extract cluster \"keywords\":\n",
    "# For each cluster, sort the cluster center’s coefficients in descending order\n",
    "# to get a list of top words that characterize that cluster.\n",
    "order_centroids = kmeans_model.cluster_centers_.argsort()[:, ::-1]\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "cluster_keywords = {}\n",
    "for i in range(num_clusters):\n",
    "    top_words = [feature_names[ind] for ind in order_centroids[i, :10]]\n",
    "    cluster_keywords[i] = top_words\n",
    "    print(f\"Cluster {i}: {' '.join(top_words)}\")\n",
    "\n",
    "# Optionally compute a distance measure to the assigned cluster center for each document.\n",
    "# Lower distance values indicate closer membership.\n",
    "distances = kmeans_model.transform(count_data)  # shape: (# docs, # clusters)\n",
    "min_distances = np.min(distances, axis=1)\n",
    "df_subset['Cluster_Distance'] = min_distances\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Produce a Legible Summary Table\n",
    "# -------------------------------\n",
    "# Build a summary table preenting:\n",
    "# - The original document index\n",
    "# - Article title for context\n",
    "# - Assigned cluster and the distance (a proxy for confidence)\n",
    "# - The cluster's top keywords\n",
    "# - A snippet of the abstract\n",
    "summary_columns = ['Article Title', 'Cluster', 'Cluster_Distance', 'Abstract']\n",
    "summary_table = df_subset[summary_columns].copy()\n",
    "\n",
    "# Add a column with the cluster keywords (joined by commas)\n",
    "summary_table['Cluster_Keywords'] = df_subset['Cluster'].apply(\n",
    "    lambda c: \", \".join(cluster_keywords[c])\n",
    ")\n",
    "# Create an abstract snippet (first 250 characters)\n",
    "summary_table['Abstract_Snippet'] = df_subset['Abstract'].fillna(\"\").apply(\n",
    "    lambda text: text if len(text) < 250 else text[:250] + \"...\"\n",
    ")\n",
    "\n",
    "print(\"Summary of Selected Documents and KMeans Clusters:\\n\")\n",
    "for idx, row in summary_table.iterrows():\n",
    "    print(f\"Document (DataFrame Index): {idx}\")\n",
    "    print(f\"Article Title    : {row['Article Title']}\")\n",
    "    print(f\"Cluster          : {row['Cluster']} (Distance: {row['Cluster_Distance']:.2f})\")\n",
    "    print(f\"Cluster Keywords : {row['Cluster_Keywords']}\")\n",
    "    print(\"Abstract Snippet :\")\n",
    "    print(row['Abstract_Snippet'])\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Visualize Cluster Distribution\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Cluster', data=df_subset, palette=\"Set2\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.title(\"Cluster Distribution for Selected Documents\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Interpretation Guidance:\n",
    "# -------------------------------\n",
    "# - Each document is assigned one of 5 clusters by the KMeans algorithm.\n",
    "# - The printed \"Cluster Keywords\" for each cluster are derived from the top words\n",
    "#   in that cluster's centroid; these can help you judge the semantics of each group.\n",
    "# - The Cluster_Distance is a measure of how close a document is to its cluster center.\n",
    "# - You can refine this pipeline by adjusting vectorizer parameters, the number of clusters,\n",
    "#   or even integrating domain-specific stopwords/seed words for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ef99b-c19f-45a4-addb-ae1eec0eed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 17\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Prepare the Text from ALL Entries\n",
    "# -------------------------------\n",
    "# Use the entire DataFrame (not just manually selected indices)\n",
    "df_all = lib_start.copy()\n",
    "\n",
    "# Combine \"Article Title\" and \"Abstract\" to capture context.\n",
    "# Replace missing values with an empty string.\n",
    "df_all['Combined_Text'] = (\n",
    "    df_all['Article Title'].fillna('') + \". \" + df_all['Abstract'].fillna('')\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Vectorize the Text using CountVectorizer \n",
    "# -------------------------------\n",
    "# CountVectorizer is preferred for LDA since it expects raw counts.\n",
    "# Adjust max_df and min_df as needed to filter out overly common and rare terms.\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", max_df=0.85, min_df=2)\n",
    "count_data = count_vectorizer.fit_transform(df_all['Combined_Text'])\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Run Topic Modeling with LDA\n",
    "# -------------------------------\n",
    "# Choose a modest number of topics (e.g., 3) based on your narrative needs.\n",
    "n_topics = 3\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)\n",
    "lda_topic_matrix = lda_model.fit_transform(count_data)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper: Extract Top Keywords for Each Topic\n",
    "# -------------------------------\n",
    "def get_topic_keywords(model, feature_names, no_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[:-no_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append(top_words)\n",
    "    return topics\n",
    "\n",
    "no_top_words = 10\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "topic_keywords = get_topic_keywords(lda_model, feature_names, no_top_words)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Annotate Each Document with the Dominant Topic\n",
    "# -------------------------------\n",
    "# For each document, determine which topic has the highest probability.\n",
    "dominant_topics = np.argmax(lda_topic_matrix, axis=1)\n",
    "max_topic_weights = np.max(lda_topic_matrix, axis=1)\n",
    "\n",
    "df_all['Dominant_Topic'] = dominant_topics\n",
    "df_all['Topic_Weight'] = max_topic_weights\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Produce a Legible Summary Table\n",
    "# -------------------------------\n",
    "# Build a summary table that includes:\n",
    "# - Document index (from the main DataFrame)\n",
    "# - Article Title for context\n",
    "# - Dominant Topic along with its weight (confidence)\n",
    "# - Top keywords for that topic\n",
    "# - A snippet of the abstract for a quick read\n",
    "summary_columns = ['Article Title', 'Dominant_Topic', 'Topic_Weight', 'Abstract']\n",
    "summary_table = df_all[summary_columns].copy()\n",
    "\n",
    "# Map each dominant topic to its top keywords, then join as a comma‑separated string.\n",
    "summary_table['Topic_Keywords'] = df_all['Dominant_Topic'].apply(\n",
    "    lambda t: \", \".join(topic_keywords[t])\n",
    ")\n",
    "\n",
    "# Create a snippet for the abstract (first 250 characters or the full abstract if shorter)\n",
    "summary_table['Abstract_Snippet'] = df_all['Abstract'].fillna(\"\").apply(\n",
    "    lambda text: text if len(text) < 250 else text[:250] + \"...\"\n",
    ")\n",
    "\n",
    "# Print out a neat summary for all documents:\n",
    "print(\"Summary of All Documents and LDA Topics:\\n\")\n",
    "for idx, row in summary_table.iterrows():\n",
    "    print(f\"Document (DataFrame Index): {idx}\")\n",
    "    print(f\"Article Title    : {row['Article Title']}\")\n",
    "    print(f\"Dominant Topic   : {row['Dominant_Topic']} (Weight: {row['Topic_Weight']:.2f})\")\n",
    "    print(f\"Topic Keywords   : {row['Topic_Keywords']}\")\n",
    "    print(\"Abstract Snippet :\")\n",
    "    print(row['Abstract_Snippet'])\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Visualize Topic Distribution Across All Documents\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Dominant_Topic', data=df_all, palette=\"Set2\")\n",
    "plt.xlabel(\"Dominant Topic\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.title(\"Topic Distribution for Entire Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Interpretation Guidance:\n",
    "# -------------------------------\n",
    "# The printed summary shows, for each document in the dataset:\n",
    "#   • The document's article title.\n",
    "#   • The dominant topic assigned to it along with the topic weight (a measure of confidence).\n",
    "#   • The top keywords for that topic.\n",
    "#   • A snippet of the abstract.\n",
    "#\n",
    "# This can help interpret themes (e.g., teaching approaches to lifecycle assessment, \n",
    "# sustainability, carbon footprints) across your entire data collection.\n",
    "#\n",
    "# You can refine this analysis by adjusting the number of topics, \n",
    "# further pre-processing the text (e.g., lemmatization), or trying different stopword lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39eef3e-1cd6-4457-a2f4-2021e059581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 18\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Prepare the Text from ALL Entries\n",
    "# -------------------------------\n",
    "# Use the entire DataFrame.\n",
    "df_all = lib_start.copy()\n",
    "\n",
    "# Combine \"Article Title\" and \"Abstract\" to capture context.\n",
    "# Replace missing values with an empty string.\n",
    "df_all['Combined_Text'] = (\n",
    "    df_all['Article Title'].fillna('') + \". \" + df_all['Abstract'].fillna('')\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Vectorize the Text using CountVectorizer \n",
    "# -------------------------------\n",
    "# CountVectorizer expects raw counts for LDA, so we filter out overly common and rare words,\n",
    "# and we use a token pattern that only allows words of 5 or more alphabetical characters.\n",
    "count_vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\", \n",
    "    max_df=0.85, \n",
    "    min_df=2,\n",
    "    token_pattern=r'\\b[a-zA-Z]{5,}\\b'\n",
    ")\n",
    "count_data = count_vectorizer.fit_transform(df_all['Combined_Text'])\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Run Topic Modeling with LDA\n",
    "# -------------------------------\n",
    "# Set the number of topics to 5.\n",
    "n_topics = 5\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)\n",
    "lda_topic_matrix = lda_model.fit_transform(count_data)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper: Extract Top Keywords for Each Topic\n",
    "# -------------------------------\n",
    "def get_topic_keywords(model, feature_names, no_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        # Get indices of the top words for this topic\n",
    "        top_words_idx = topic.argsort()[:-no_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append(top_words)\n",
    "    return topics\n",
    "\n",
    "no_top_words = 10\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "topic_keywords = get_topic_keywords(lda_model, feature_names, no_top_words)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Annotate Each Document with the Dominant Topic\n",
    "# -------------------------------\n",
    "# For each document, determine which topic has the highest probability.\n",
    "dominant_topics = np.argmax(lda_topic_matrix, axis=1)\n",
    "max_topic_weights = np.max(lda_topic_matrix, axis=1)\n",
    "\n",
    "df_all['Dominant_Topic'] = dominant_topics\n",
    "df_all['Topic_Weight'] = max_topic_weights\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Produce a Legible Summary Table\n",
    "# -------------------------------\n",
    "# Build a summary table that includes:\n",
    "# - Document index (from the main DataFrame)\n",
    "# - Article Title for context\n",
    "# - Dominant Topic along with its weight\n",
    "# - Top keywords for that topic\n",
    "# - A snippet of the abstract for quick inspection\n",
    "summary_columns = ['Article Title', 'Dominant_Topic', 'Topic_Weight', 'Abstract']\n",
    "summary_table = df_all[summary_columns].copy()\n",
    "\n",
    "# Map each dominant topic to its top keywords, then join as a comma-separated string.\n",
    "summary_table['Topic_Keywords'] = df_all['Dominant_Topic'].apply(\n",
    "    lambda t: \", \".join(topic_keywords[t])\n",
    ")\n",
    "\n",
    "# Create an abstract snippet (first 250 characters)\n",
    "summary_table['Abstract_Snippet'] = df_all['Abstract'].fillna(\"\").apply(\n",
    "    lambda text: text if len(text) < 250 else text[:250] + \"...\"\n",
    ")\n",
    "\n",
    "print(\"Summary of All Documents and LDA Topics:\\n\")\n",
    "for idx, row in summary_table.iterrows():\n",
    "    print(f\"Document (DataFrame Index): {idx}\")\n",
    "    print(f\"Article Title    : {row['Article Title']}\")\n",
    "    print(f\"Dominant Topic   : {row['Dominant_Topic']} (Weight: {row['Topic_Weight']:.2f})\")\n",
    "    print(f\"Topic Keywords   : {row['Topic_Keywords']}\")\n",
    "    print(\"Abstract Snippet :\")\n",
    "    print(row['Abstract_Snippet'])\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Visualize Topic Distribution Across All Documents\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Dominant_Topic', data=df_all, palette=\"Set2\")\n",
    "plt.xlabel(\"Dominant Topic\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.title(\"Topic Distribution for Entire Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Interpretation Guidance:\n",
    "# -------------------------------\n",
    "# The printed summary lists each document with:\n",
    "#   • Its article title,\n",
    "#   • The dominant topic (with a confidence weight),\n",
    "#   • The top keywords associated with that topic,\n",
    "#   • And a snippet of the abstract.\n",
    "#\n",
    "# This can help in a qualitative evaluation of how well the topics align with themes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37caf6c-a3b9-4192-a534-755cb5e71e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 19\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Prepare the Text for All Entries\n",
    "# -------------------------------\n",
    "# Use the entire DataFrame (assumed to be lib_start)\n",
    "# Fill missing abstracts with an empty string.\n",
    "documents = lib_start[\"Abstract\"].fillna(\"\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Text Vectorization with TF‑IDF\n",
    "# -------------------------------\n",
    "# We use a Count TF‑IDF Vectorizer with English stop words.\n",
    "# Adjust max_df as needed to exclude overly common words.\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.85)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3a: Clustering with KMeans\n",
    "# -------------------------------\n",
    "# Choose a number of clusters. For demonstration, we use 3.\n",
    "num_clusters = 3\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans_model.fit_predict(X)\n",
    "# Add cluster labels to the main DataFrame for reference.\n",
    "lib_start[\"Cluster\"] = cluster_labels\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3b: Dimensionality Reduction (PCA)\n",
    "# -------------------------------\n",
    "# Use PCA to reduce the high-dimensional TF‑IDF features to 2 dimensions for visualization.\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "# Plot the clusters on a 2D scatter plot.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0],\n",
    "                y=X_pca[:, 1],\n",
    "                hue=cluster_labels,\n",
    "                palette=\"deep\",\n",
    "                s=100)\n",
    "\n",
    "# Annotate each point with its original DataFrame index.\n",
    "for i, idx in enumerate(lib_start.index):\n",
    "    plt.text(X_pca[i, 0] + 0.01, X_pca[i, 1] + 0.01, str(idx), fontsize=9)\n",
    "\n",
    "plt.title(\"PCA of Abstracts with KMeans Clusters\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Topic Modeling with Latent Dirichlet Allocation (LDA)\n",
    "# -------------------------------\n",
    "# Apply LDA on the same TF‑IDF data to extract latent topics.\n",
    "n_topics = 3\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_topic_matrix = lda.fit_transform(X)\n",
    "\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        # Get indices of the top words for this topic.\n",
    "        top_features_ind = topic.argsort()[:-num_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        print(f\"Topic {topic_idx}: {' '.join(top_features)}\")\n",
    "\n",
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Extracted Topics:\")\n",
    "display_topics(lda, feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca590c8-7e28-44dd-8060-4b9e95853848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 20\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Define a set of US state abbreviations.\n",
    "US_STATES = {\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\",\n",
    "    \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\",\n",
    "    \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\",\n",
    "    \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\",\n",
    "    \"WI\", \"WY\"\n",
    "}\n",
    "\n",
    "def standardize_country(country):\n",
    "    \"\"\"\n",
    "    Standardizes a country token:\n",
    "      - If it exactly matches 'usa', 'united states', etc.\n",
    "      - Or if it is a known US state abbreviation (typically 2 letters),\n",
    "        then return 'USA'.\n",
    "    \"\"\"\n",
    "    token = country.strip()\n",
    "    # Check for variations of USA\n",
    "    if token.lower() in {\"usa\", \"united states\", \"u.s.a.\", \"us\"}:\n",
    "        return \"USA\"\n",
    "    # If token is 2 letters and is a known US state, treat it as USA.\n",
    "    if len(token) == 2 and token.upper() in US_STATES:\n",
    "        return \"USA\"\n",
    "    return token\n",
    "\n",
    "def extract_countries(address):\n",
    "    \"\"\"\n",
    "    Given an address string that may contain multiple addresses separated by semicolons,\n",
    "    extract the country (assumed to be the substring after the last comma in each address),\n",
    "    and standardize it.\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return []\n",
    "    # Split the address into individual addresses.\n",
    "    addresses = address.split(\";\")\n",
    "    countries = []\n",
    "    for addr in addresses:\n",
    "        parts = addr.split(',')\n",
    "        if len(parts) > 1:\n",
    "            # Take the last element and standardize it.\n",
    "            country = standardize_country(parts[-1])\n",
    "            if country:\n",
    "                countries.append(country)\n",
    "    return countries\n",
    "\n",
    "# Apply extraction and standardization on the 'Addresses' column.\n",
    "lib_start['Countries'] = lib_start['Addresses'].apply(extract_countries)\n",
    "\n",
    "# Aggregate counts: go through each row's list of countries and count occurrences.\n",
    "country_counter = Counter()\n",
    "for country_list in lib_start['Countries'].dropna():\n",
    "    country_counter.update(country_list)\n",
    "\n",
    "# (Optional) Print out the counter to verify that all US addresses are now combined.\n",
    "# print(country_counter)\n",
    "\n",
    "# Convert the counter to a DataFrame.\n",
    "country_counts_df = pd.DataFrame.from_dict(country_counter, orient='index', columns=['Author_Count'])\n",
    "country_counts_df = country_counts_df.reset_index().rename(columns={'index': 'Country'})\n",
    "\n",
    "# Optional: Remove any empty strings (if any) from the country counts.\n",
    "country_counts_df = country_counts_df[country_counts_df['Country'] != \"\"]\n",
    "\n",
    "# Create a bubble map using Plotly Express.\n",
    "fig = px.scatter_geo(\n",
    "    country_counts_df,\n",
    "    locations=\"Country\",            # Use the country names for locations.\n",
    "    locationmode=\"country names\",   # Match based on full country names.\n",
    "    size=\"Author_Count\",            # Bubble size based on count of authors.\n",
    "    text=\"Author_Count\",            # Display the count inside the bubble.\n",
    "    size_max=50,                    # Maximum bubble size.\n",
    "#    projection=\"natural earth\"      # A pleasant global projection.\n",
    "    projection=\"robinson\"      # A pleasant global projection.\n",
    "\n",
    ")\n",
    "\n",
    "# Adjust text to be centered within the bubbles.\n",
    "fig.update_traces(textposition='middle center', marker=dict(sizemode='area'))\n",
    "\n",
    "# Remove the graph title and customize the layout minimally.\n",
    "fig.update_layout(\n",
    "    title_text=\"\", \n",
    "    geo=dict(\n",
    "        showland=True, \n",
    "        landcolor=\"LightGreen\",\n",
    "        lakecolor='white'\n",
    "        \n",
    "    ),\n",
    "    margin=dict(l=0, r=0, t=0, b=0)\n",
    ")\n",
    "#---------------------------------\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    paper_bgcolor=\"black\",       # Sets the background outside the map.\n",
    "    plot_bgcolor=\"black\"         # Sets the background for the plotting area.\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a12b7e-ebb2-4129-863b-6d5bcc8f482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 21\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import plotly.express as px\n",
    "\n",
    "# Define a set of US state abbreviations.\n",
    "US_STATES = {\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\",\n",
    "    \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\",\n",
    "    \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\",\n",
    "    \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\",\n",
    "    \"WI\", \"WY\"\n",
    "}\n",
    "\n",
    "def standardize_country(country):\n",
    "    \"\"\"\n",
    "    Clean and standardize a token:\n",
    "      - Remove extraneous punctuation.\n",
    "      - If the token exactly matches common USA expressions, return \"USA\".\n",
    "      - If the token is exactly two letters and in US_STATES, return \"USA\".\n",
    "      - If the token starts with a US state abbreviation (e.g., \"CA 94043\"),\n",
    "        return \"USA\".\n",
    "      - Otherwise, return the token in title-case.\n",
    "    \"\"\"\n",
    "    token = country.strip().replace('.', '')\n",
    "    token_upper = token.upper()\n",
    "    \n",
    "    # Check for explicit variants of USA.\n",
    "    if token_upper in {\"USA\", \"UNITED STATES\", \"US\", \"U S A\"}:\n",
    "        return \"USA\"\n",
    "    \n",
    "    # If token is exactly two letters and a known US state.\n",
    "    if re.fullmatch(r\"[A-Z]{2}\", token_upper) and token_upper in US_STATES:\n",
    "        return \"USA\"\n",
    "    \n",
    "    # If token has multiple parts, check if the first token is a US state abbreviation.\n",
    "    tokens = token_upper.split()\n",
    "    if tokens and tokens[0] in US_STATES:\n",
    "        return \"USA\"\n",
    "    \n",
    "    return token.title()\n",
    "\n",
    "def extract_countries(address):\n",
    "    \"\"\"\n",
    "    Given an address string that may contain multiple addresses separated by semicolons,\n",
    "    extract the country (assumed to be the substring after the last comma in each address),\n",
    "    and standardize it.\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return []\n",
    "    addresses = address.split(\";\")\n",
    "    countries = []\n",
    "    for addr in addresses:\n",
    "        parts = addr.split(',')\n",
    "        if len(parts) > 1:\n",
    "            candidate = parts[-1]\n",
    "            standardized = standardize_country(candidate)\n",
    "            if standardized:\n",
    "                countries.append(standardized)\n",
    "    return countries\n",
    "\n",
    "# Apply extraction and standardization on the 'Addresses' column.\n",
    "lib_start['Countries'] = lib_start['Addresses'].apply(extract_countries)\n",
    "\n",
    "# Aggregate counts: go through each row's list of countries and count occurrences.\n",
    "country_counter = Counter()\n",
    "for country_list in lib_start['Countries'].dropna():\n",
    "    country_counter.update(country_list)\n",
    "\n",
    "# Convert the counter to a DataFrame.\n",
    "country_counts_df = pd.DataFrame.from_dict(country_counter, orient='index', columns=['Author_Count'])\n",
    "country_counts_df = country_counts_df.reset_index().rename(columns={'index': 'Country'})\n",
    "country_counts_df = country_counts_df[country_counts_df['Country'] != \"\"]\n",
    "\n",
    "# Create a bubble map using Plotly Express.\n",
    "fig = px.scatter_geo(\n",
    "    country_counts_df,\n",
    "    locations=\"Country\",            # Country names for locations.\n",
    "    locationmode=\"country names\",   # Match based on full country names.\n",
    "    size=\"Author_Count\",            # Bubble size based on count of authors.\n",
    "    text=\"Author_Count\",            # Display the count inside the bubble.\n",
    "    size_max=50,                    # Maximum bubble size.\n",
    "    projection=\"robinson\"           # A pleasant global projection.\n",
    ")\n",
    "\n",
    "# Adjust text to be centered within the bubbles.\n",
    "fig.update_traces(textposition='middle center', marker=dict(sizemode='area'))\n",
    "\n",
    "# Customize the layout minimally.\n",
    "fig.update_layout(\n",
    "    title_text=\"\",\n",
    "    geo=dict(\n",
    "        showland=True,\n",
    "        landcolor=\"LightGreen\",\n",
    "        lakecolor='white'\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    paper_bgcolor=\"black\",       # Sets the background outside the map.\n",
    "    plot_bgcolor=\"black\"         # Sets the background for the plotting area.\n",
    ")\n",
    "\n",
    "# Display the figure.\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824f699-2c38-4752-b4ed-b19f5c466063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 22\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import plotly.express as px\n",
    "\n",
    "# Define a set of US state abbreviations.\n",
    "US_STATES = {\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\",\n",
    "    \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\",\n",
    "    \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\",\n",
    "    \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\",\n",
    "    \"WI\", \"WY\"\n",
    "}\n",
    "\n",
    "def standardize_country(country):\n",
    "    \"\"\"\n",
    "    Clean and standardize a token:\n",
    "      - Remove extraneous punctuation.\n",
    "      - If the token exactly matches common USA expressions, return \"USA\".\n",
    "      - If the token is exactly two letters and in US_STATES, return \"USA\".\n",
    "      - If the token starts with a US state abbreviation (e.g., \"CA 94043\"),\n",
    "        return \"USA\".\n",
    "      - Otherwise, return the token in title-case.\n",
    "    \"\"\"\n",
    "    token = country.strip().replace('.', '')\n",
    "    token_upper = token.upper()\n",
    "    \n",
    "    # Check for explicit variants of USA.\n",
    "    if token_upper in {\"USA\", \"UNITED STATES\", \"US\", \"U S A\"}:\n",
    "        return \"USA\"\n",
    "    \n",
    "    # If token is exactly two letters and a known US state.\n",
    "    if re.fullmatch(r\"[A-Z]{2}\", token_upper) and token_upper in US_STATES:\n",
    "        return \"USA\"\n",
    "    \n",
    "    # If token has multiple parts, check if the first token is a US state abbreviation.\n",
    "    tokens = token_upper.split()\n",
    "    if tokens and tokens[0] in US_STATES:\n",
    "        return \"USA\"\n",
    "    \n",
    "    return token.title()\n",
    "\n",
    "def extract_countries(address):\n",
    "    \"\"\"\n",
    "    Given an address string that may contain multiple addresses separated by semicolons,\n",
    "    extract the country (assumed to be the substring after the last comma in each address),\n",
    "    and standardize it.\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return []\n",
    "    addresses = address.split(\";\")\n",
    "    countries = []\n",
    "    for addr in addresses:\n",
    "        parts = addr.split(',')\n",
    "        if len(parts) > 1:\n",
    "            candidate = parts[-1]\n",
    "            standardized = standardize_country(candidate)\n",
    "            if standardized:\n",
    "                countries.append(standardized)\n",
    "    return countries\n",
    "\n",
    "# Apply extraction and standardization on the 'Addresses' column.\n",
    "lib_start['Countries'] = lib_start['Addresses'].apply(extract_countries)\n",
    "\n",
    "# Aggregate counts: go through each row's list of countries and count occurrences.\n",
    "country_counter = Counter()\n",
    "for country_list in lib_start['Countries'].dropna():\n",
    "    country_counter.update(country_list)\n",
    "\n",
    "# Convert the counter to a DataFrame.\n",
    "country_counts_df = pd.DataFrame.from_dict(\n",
    "    country_counter, orient='index', columns=['Author_Count']\n",
    ")\n",
    "country_counts_df = country_counts_df.reset_index().rename(columns={'index': 'Country'})\n",
    "country_counts_df = country_counts_df[country_counts_df['Country'] != \"\"]\n",
    "\n",
    "# Create a bubble map using Plotly Express.\n",
    "fig = px.scatter_geo(\n",
    "    country_counts_df,\n",
    "    locations=\"Country\",            # Use the country names for locations.\n",
    "    locationmode=\"country names\",   # Match based on full country names.\n",
    "    size=\"Author_Count\",            # Bubble size based on count of authors.\n",
    "    text=\"Author_Count\",            # Display the count inside the bubble.\n",
    "    size_max=50,                    # Maximum bubble size.\n",
    "    projection=\"mercator\"           # A pleasant global projection.\n",
    ")\n",
    "\n",
    "# Adjust text to be centered within the bubbles.\n",
    "fig.update_traces(textposition='middle center', marker=dict(sizemode='area'))\n",
    "\n",
    "# Add country borders by updating the geo layout.\n",
    "fig.update_layout(\n",
    "    title_text=\"\",\n",
    "    geo=dict(\n",
    "        showland=True,\n",
    "        landcolor=\"LightGreen\",\n",
    "        lakecolor='white',\n",
    "        showcountries=True,    # Enable country borders.\n",
    "        countrycolor=\"white\",  # Border color.\n",
    "        countrywidth=1         # Border line thickness.\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, t=0, b=0)\n",
    ")\n",
    "\n",
    "# Set the overall background.\n",
    "fig.update_layout(\n",
    "    paper_bgcolor=\"black\",       # Sets the background outside the map.\n",
    "    plot_bgcolor=\"black\"         # Sets the background for the plotting area.\n",
    ")\n",
    "\n",
    "# Display the figure.\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a35cd9-51a4-42b2-83f7-5e754ecc96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 23\n",
    "\n",
    "# --------------------\n",
    "# Step 1: Function to extract and standardize country names\n",
    "# --------------------\n",
    "def extract_countries(address):\n",
    "    \"\"\"\n",
    "    Given an address string that may contain multiple addresses separated by semicolons,\n",
    "    extract the country (assumed to be the substring after the last comma in each address).\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return []\n",
    "    addresses = address.split(\";\")\n",
    "    countries = []\n",
    "    for addr in addresses:\n",
    "        parts = addr.split(',')\n",
    "        if len(parts) >= 2:\n",
    "            country = parts[-1].strip()\n",
    "            if country:\n",
    "                countries.append(country)\n",
    "    return countries\n",
    "\n",
    "def standardize_country(country):\n",
    "    \"\"\"\n",
    "    Standardizes country names:\n",
    "      - Converts various representations of the United States to 'USA'\n",
    "    \"\"\"\n",
    "    if country.lower() in [\"usa\", \"united states\", \"u.s.a.\", \"us\"]:\n",
    "        return \"USA\"\n",
    "    return country\n",
    "\n",
    "# Apply extraction and standardization on the 'Addresses' column.\n",
    "lib_start['Countries'] = lib_start['Addresses'].apply(lambda addr: [standardize_country(c) for c in extract_countries(addr)])\n",
    "\n",
    "# --------------------\n",
    "# Step 2: Aggregate author counts by country\n",
    "# --------------------\n",
    "all_countries = []\n",
    "for country_list in lib_start['Countries'].dropna():\n",
    "    all_countries.extend(country_list)\n",
    "\n",
    "# Use a Counter to sum up the occurrences.\n",
    "country_counter = Counter(all_countries)\n",
    "\n",
    "# Convert the counter into a DataFrame.\n",
    "df_country_counts = pd.DataFrame.from_dict(country_counter, orient='index', columns=['Author_Count']).reset_index()\n",
    "df_country_counts = df_country_counts.rename(columns={'index': 'Country'})\n",
    "df_country_counts = df_country_counts[df_country_counts['Country'] != \"\"]  # Remove any empty strings\n",
    "\n",
    "# --------------------\n",
    "# Step 3: Create a Bubble Map (Static Image)\n",
    "# --------------------\n",
    "fig = px.scatter_geo(\n",
    "    df_country_counts,\n",
    "    locations=\"Country\",          # Country names for locations.\n",
    "    locationmode=\"country names\", # Use full country names.\n",
    "    size=\"Author_Count\",          # Bubble size corresponds to author count.\n",
    "    text=\"Author_Count\",          # Display the count inside the bubble.\n",
    "    projection=\"natural earth\",   # Global projection.\n",
    "    size_max=100                  # Adjust maximum bubble size if needed.\n",
    ")\n",
    "\n",
    "# Enlarge the map for clarity.\n",
    "fig.update_layout(width=1200, height=800)\n",
    "\n",
    "# Position text labels above the bubbles.\n",
    "fig.update_traces(textposition=\"top center\", marker=dict(sizemode='area'))\n",
    "\n",
    "# Disable hover effects for a static look.\n",
    "fig.update_traces(hoverinfo=\"skip\")\n",
    "\n",
    "# Save the map as a static image, explicitly specifying kaleido as the engine.\n",
    "fig.write_image(\"static_bubble_map.png\", engine=\"kaleido\")\n",
    "\n",
    "# Optionally display the static image in the notebook.\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9dd2f-f7f5-4c08-9f78-122fe7cf5647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b58398-b7eb-4917-9c78-b8b71231d520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
